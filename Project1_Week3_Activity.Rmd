---
title: "MA214 Applied Statistics - Project 1 Week 3 Activity"
subtitle: "Modeling, Model Selection, and Diagnostics"
author: "Section C4  |  Group 7"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: true
  html_document:
    toc: true
    toc_float: true
    code_folding: show
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(ggplot2)
library(dplyr)
library(tidyr)
library(broom)
library(car)  # For VIF and diagnostic plots
```

## Overview

**Today's Focus:** Fitting regression models, selecting the best model,
and validating model assumptions.

**Important:** This worksheet covers BOTH linear regression and logistic
regression. **Choose the section that matches your project's response
variable:**

-   **Section A (Linear Regression):** For continuous/numeric response
    variables (e.g., price, temperature, age)
-   **Section B (Logistic Regression):** For binary response variables
    (e.g., yes/no, success/failure, 0/1)

**What you should have ready:** - Your cleaned dataset - Summary
statistics and EDA from Week 2 - Initial ideas about which variables to
include

**By the end of today, you should have:** - Fitted at least 2-3
regression models - Selected your best model with justification -
Checked model diagnostics - Identified any problems and potential
solutions

------------------------------------------------------------------------

## Part 0: Load Your Data

```{r load-data}
# Load dataset
data <- read.csv("ai_dev_productivity.csv")

# Recode task_success as factor (same as Week 2)
data$task_success <- factor(data$task_success, levels = c(0, 1), labels = c("Failure", "Success"))

# Recreate composite productivity score from Week 2
data <- data |>
  mutate(
    productivity = scale(commits) - scale(bugs_reported) + scale(as.numeric(task_success))
  )
data$productivity <- as.numeric(data$productivity)

# Display first few rows
head(data)

# Check structure
str(data)
```

**Group Discussion:**

\- What is your response variable?

-   Our response variable is `productivity`, a composite score built
    from z-scores of commits, bugs_reported, and task_success (from Week
    2).

\- Is it continuous (use Section A) or binary (use Section B)?

-   It is continuous (numeric), so we use Section A (Linear Regression).

\- What predictors are you considering?

-   `hours_coding`, `sleep_hours`, `cognitive_load`, `ai_usage_hours`,
    `coffee_intake_mg`, and `distractions`. These were identified in
    Week 2's EDA as having meaningful correlations with productivity.

\- Are all variables in the correct format (numeric, factor, etc.)?

-   Yes. All predictors are numeric, which is correct for linear
    regression. `task_success` was recoded to a factor but is only used
    inside the productivity score, not as a standalone predictor.

**☐ Check here which section you'll use:**

-   [x] Section A: Linear Regression

-   [ ] Section B: Logistic Regression

------------------------------------------------------------------------

# SECTION A: LINEAR REGRESSION

**Use this section if your response variable is continuous (numeric).**

**Skip to Section B if you have a binary response variable.**

------------------------------------------------------------------------

## A1: Fitting Linear Regression Models

### Model 1: Simple Linear Regression

**Objective:** Start with a simple model using your most important
predictor.

```{r a-model1}
# Simple linear regression: productivity predicted by hours_coding
model1 <- lm(productivity ~ hours_coding, data = data)

summary(model1)
```

**Questions to answer:**

1.  What is the adj R² value? What does it mean?
    -   **Your answer:** The Adjusted R² is 0.3642, meaning hours_coding
        alone explains about 36.4% of the variance in productivity. This
        is a moderate fit — hours_coding is a meaningful predictor on
        its own, but a lot of variance remains unexplained.
2.  How do you interpret the slope coefficient?
    -   **Your answer:** The slope for hours_coding is 0.6195. This
        means that for each additional hour spent coding, productivity
        score increases by about 0.62 points on average. The
        relationship is statistically significant (p \< 2e-16).

**Visualize the model:**

```{r a-model1-plot}
ggplot(data, aes(x = hours_coding, y = productivity)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE, color = "blue") +
  labs(title = "Model 1: Simple Linear Regression",
       x = "Hours Coding",
       y = "Productivity Score") +
  theme_minimal()
```

------------------------------------------------------------------------

### Model 2: Multiple Linear Regression (Main Effects)

**Objective:** Add more predictors to improve the model.

```{r a-model2}
# Multiple regression with top 4 predictors from Week 2 EDA
model2 <- lm(productivity ~ hours_coding + sleep_hours +
               cognitive_load + ai_usage_hours, data = data)

summary(model2)
```

**Questions to answer:**

1.  What is the R² value? How does it compare to Model 1?

    -   **Your answer:** The R² is 0.4516, up from 0.3655 in Model 1.
        Adding sleep_hours, cognitive_load, and ai_usage_hours explains
        about 8.6% more variance in productivity.

2.  What is the Adjusted R² value? Why is this important?

    -   **Your answer:** The Adjusted R² is 0.4472, up from 0.3642 in
        Model 1. Adjusted R² penalizes for extra predictors, so the fact
        that it still increased substantially confirms the added
        variables genuinely improved the model. Notably, hours_coding (p
        \< 2e-16) and sleep_hours (p = 9.52e-06) are highly significant,
        ai_usage_hours is significant (p = 0.006), while cognitive_load
        is not significant (p = 0.19).

------------------------------------------------------------------------

### Model 3: Model with Interactions

**Objective:** Test if relationships between variables depend on each
other.

```{r a-model3}
# Interaction model: does the effect of hours_coding depend on AI usage?
# Week 2 showed devs who both code more AND use AI more had highest productivity
model3 <- lm(productivity ~ hours_coding * ai_usage_hours +
               sleep_hours + cognitive_load, data = data)

summary(model3)
```

**Visualize the interaction:**

```{r a-interaction-plot}
data %>%
  mutate(ai_group = cut(ai_usage_hours, breaks = 3,
                         labels = c("Low", "Medium", "High"))) %>%
  ggplot(aes(x = hours_coding, y = productivity, color = ai_group)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(title = "Interaction: Hours Coding x AI Usage",
       x = "Hours Coding",
       y = "Productivity Score",
       color = "AI Usage Level") +
  theme_minimal()
```

------------------------------------------------------------------------

### Model 4: Additional model to compare

**Objective:** Test if additional model can be used

```{r a-model4}
# Full model with all 6 predictors (including coffee and distractions)
model4 <- lm(productivity ~ hours_coding + sleep_hours +
               cognitive_load + ai_usage_hours +
               coffee_intake_mg + distractions, data = data)

summary(model4)

# Check multicollinearity with VIF
vif(model4)
```

**Visualize the model:**

```{r model-4-visual}
# Compare actual vs predicted values for the full model
data$predicted4 <- predict(model4)

ggplot(data, aes(x = predicted4, y = productivity)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0,
              color = "red", linetype = "dashed") +
  labs(title = "Model 4: Actual vs Predicted Productivity",
       x = "Predicted Productivity",
       y = "Actual Productivity") +
  theme_minimal()
```

------------------------------------------------------------------------

## A2: Model Selection (Linear)

```{r a-model-comparison}
# Create comparison using broom::glance
 library(broom)
 bind_rows(
   glance(model1) %>% mutate(model = "Model 1"),
   glance(model2) %>% mutate(model = "Model 2"),
   glance(model3) %>% mutate(model = "Model 3"),
   glance(model4) %>% mutate(model = "Model 4")
 ) %>%
   select(model, r.squared, adj.r.squared, AIC)
```

**Model Selection Table:**

| Criterion       | Best Model | Value     |
|-----------------|------------|-----------|
| Highest Adj. R² | Model 4    | 0.4697183 |
| R²              | Model 4    | 0.47609   |
| AIC             | Model 4    | 1801.57   |

**Your chosen model and justification: Model 4 is the preferred model
because it has the highest R² and adjusted R², meaning it explains the
greatest proportion of variability in the response variable.
Additionally, it has the lowest AIC, indicating the best balance between
model fit and complexity. Since it performs best across all comparison
metrics, Model 4 is the strongest overall model.**

------------------------------------------------------------------------

## A3: Model Diagnostics (Linear)

**The four key assumptions:** 1. **Linearity** 2. **Independence** 3.
**Normality** of residuals 4. **Equal Variance**

### All diagnostic plots at once:

```{r a-diagnostics-all}
# Create all 4 diagnostic plots
par(mfrow = c(2, 2))
plot(model4)  # Replace with your chosen model
par(mfrow = c(1, 1))
```

### Diagnostic Plot 1: Residuals vs Fitted

```{r a-resid-fitted}
 plot(model4, which = 1)

# Or using ggplot2
 augment(model4) %>%
   ggplot(aes(x = .fitted, y = .resid)) +
   geom_point(alpha = 0.6) +
   geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
   geom_smooth(se = TRUE, color = "blue") +
   labs(title = "Residuals vs Fitted Values") +
   theme_minimal()
```

**What to look for:**

\- ✓ **Good:** Random scatter around zero

\- ✗ **Bad:** Curved pattern (non-linearity), funnel shape (unequal
variance)

**Your observations: The residuals are generally centered around zero,
but there is a slight curved pattern in the smooth line. This suggests a
slight violation of the linearity assumption. However, there is no
strong funnel shape, so the variance of the residuals appears relatively
constant. Overall, the model assumptions are mostly reasonable, though
there is some slight non-linearity.**

------------------------------------------------------------------------

### Diagnostic Plot 2: Normal Q-Q Plot

```{r a-qq-plot}
 plot(model4, which = 2)
```

**Your observations: Most of the points closely follow the straight
reference line, indicating that the residuals are about normally
distributed. However, there are slight deviations in the lower tail,
where a few points fall below the line. This suggests mild departures
from normality, probably due to a few outliers. Overall, the normality
assumption appears reasonably satisfied.**

------------------------------------------------------------------------

## A4: Addressing Issues (Linear)

### If you found non-linearity:

```{r a-fix-nonlinearity}
# Add polynomial term
 model_poly <- lm(productivity ~ hours_coding + I(hours_coding^2) * ai_usage_hours + sleep_hours + cognitive_load + coffee_intake_mg + distractions, data = data)
# Or log transformation
# model_log <- lm(log(productivity) ~ hours_coding * ai_usage_hours +
#               sleep_hours + cognitive_load, data = data)

# Create comparison using broom::glance
 library(broom)
 bind_rows(
   glance(model_poly) %>% mutate(model = "Model poly"),
   glance(model4) %>% mutate(model = "Model 4")
 ) %>%
   select(model, r.squared, adj.r.squared, AIC)
# plot model_poly
 plot(model_poly, which = 1)
 augment(model_poly) %>%
   ggplot(aes(x = .fitted, y = .resid)) +
   geom_point(alpha = 0.6) +
   geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
   geom_smooth(se = TRUE, color = "blue") +
   labs(title = "model_poly Residuals vs Fitted Values") +
   theme_minimal()
 
 #model 4
  plot(model4, which = 1)
 augment(model4) %>%
   ggplot(aes(x = .fitted, y = .resid)) +
   geom_point(alpha = 0.6) +
   geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
   geom_smooth(se = TRUE, color = "blue") +
   labs(title = "model4 Residuals vs Fitted Values") +
   theme_minimal()
```

### If you found unequal variance:

```{r a-fix-heteroscedasticity}
# Log transformation often helps
# model_log_y <- lm(log(response) ~ predictor1 + predictor2, data = data)
```

------------------------------------------------------------------------

## A5: Final Linear Model

```{r a-final-model}
# Your final model
# Model 4 was selected as the final model. While we tested adding a polynomial term for hours_coding to account for possible nonlinearity, this modification did not noticeably improve R², adjusted R², or AIC. The residuals of Model 4 show only minor deviations from linearity, so the simpler, fully linear Model 4 provides a good balance between accuracy and interpretability. 
 final_model <- lm(productivity ~ hours_coding + sleep_hours +
               cognitive_load + ai_usage_hours +
               coffee_intake_mg + distractions, data = data)

 summary(final_model)
```

**Model equation:**

$$\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...$$

**Your equation:**
$$\hat{productivity} = -6.311+ 0.384(hours\_coding) + 0.390 (sleep\_hours) + 0.012(cognitive\_load) - 0.167 (ai\_usage\_hours) + 0.00454(coffee\_intake\_mg)- 0.081(distractions)$$

**Performance:**

\- R² = 0.4761

\- Adjusted R² = 0.4697

\- RMSE = 1.453

\- AIC = 1801.57

**Diagnostics checklist:**

[KINDA ] Linearity

I would say the model is only violating the assumption of linearity
slightly.

[YES] Normality

[YES] Independence

[YES]Equal variance

------------------------------------------------------------------------

## Checklist for Today

Before you leave, make sure you have:

[YES] Identified whether you're using linear or logistic regression

[YES] Fitted at least 2-3 different models

[YES] Compared models using appropriate criteria

[YES] Selected your best model with clear justification

[YES] Created all relevant diagnostic plots

[YES] Checked and interpreted diagnostic results

[YES] Calculated performance metrics (R²/RMSE )

[YES] Addressed any major issues (or documented why you can't)

[YES] Written out your final model equation

[YES] Interpreted at least 2 key coefficients/odds ratios

------------------------------------------------------------------------

## Resources and Tips

### For Linear Regression:

**Interpreting Coefficients:** - Continuous predictor: "A one-unit
increase in X is associated with a β-unit change in Y" - Categorical
predictor: "Compared to [reference], this category has β units
higher/lower Y" - Log-transformed Y: "A one-unit increase in X is
associated with approximately 100×β% change in Y"

**Model Selection:** - Don't just pick highest R²! - Use Adjusted R² for
comparing models with different numbers of predictors - Lower AIC/BIC is
better - Consider interpretability

### For Logistic Regression:

**Interpreting Odds Ratios:** - OR = 1: No association - OR \> 1:
Positive association (predictor increases odds of success) - OR \< 1:
Negative association (predictor decreases odds of success) - Example: OR
= 2.5 means "the odds of success are 2.5 times higher"

**Model Selection:** - Lower AIC is better

**Common Issues:** - Separation: Some predictors perfectly predict
outcome → use penalized regression

\- Non-linearity in logit: Add polynomial terms or categorize continuous
predictors

\- Poor calibration: Model may discriminate well but probabilities are
off

------------------------------------------------------------------------

## Submission

**Submit the following to Blackboard:**

1\. This R Markdown file (`.Rmd`)

2\. Knitted PDF

3\. Your dataset

4\. Any additional R scripts if needed

**One submission per group.**

\+

**Individual submission.**

------------------------------------------------------------------------

## Group Information

| Name         | BUID      | Present/Absent |
|--------------|-----------|----------------|
| Hibak Hussen | U15515562 | Present        |
| Ian Sabia    | U33871576 | Present        |
| Muze Ren     | U21890514 | Present        |
| Yunhao Zhou  | U18926707 | Present        |

**Which section did you use?**

-   [x] Section A: Linear Regression

-   [ ] Section B: Logistic Regression

**Group Notes:**

(Use this space for any additional notes or decisions made during the
lab)
